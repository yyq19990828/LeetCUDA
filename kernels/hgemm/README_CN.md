## âš¡ï¸âš¡ï¸Toy-HGEMM: å®ç° cuBLAS 98%~100% çš„ TFLOPS ğŸ‰ğŸ‰

![toy-hgemm-library](https://github.com/user-attachments/assets/962bda14-b494-4423-b8eb-775da9f5503d)

[ğŸ“–Toy-HGEMM Libraryâš¡ï¸âš¡ï¸](./kernels/hgemm) æ˜¯ä¸€ä¸ªä»å¤´å¼€å§‹ä½¿ç”¨ Tensor Cores (WMMA, MMA PTX å’Œ CuTe API) ç¼–å†™äº†è®¸å¤š HGEMM å†…æ ¸çš„åº“ï¼Œå› æ­¤å¯ä»¥è¾¾åˆ° **cuBLAS** `98%~100%` çš„æ€§èƒ½ã€‚è¿™é‡Œçš„ä»£ç æºè‡ª ğŸ“–[CUDA-Learn-Notes](https://github.com/xlite-dev/CUDA-Learn-Notes) ![](https://img.shields.io/github/stars/xlite-dev/CUDA-Learn-Notes.svg?style=social) å¹¶ä½œä¸ºä¸€ä¸ªç‹¬ç«‹çš„åº“å¯¼å‡ºï¼Œè¯·æŸ¥çœ‹ [CUDA-Learn-Notes](https://github.com/xlite-dev/CUDA-Learn-Notes) è·å–æœ€æ–°æ›´æ–°ã€‚æ¬¢è¿ ğŸŒŸğŸ‘†ğŸ»star è¿™ä¸ªä»“åº“æ¥æ”¯æŒæˆ‘ï¼Œéå¸¸æ„Ÿè°¢ ~ ğŸ‰ğŸ‰

<div id="hgemm-sgemm"></div>

<div align='center'>
  <img src='https://github.com/user-attachments/assets/71927ac9-72b3-4ce9-b0e2-788b5885bc99' height="170px" width="270px">
  <img src='https://github.com/user-attachments/assets/05ef4f5e-d999-48ea-b58e-782cffb24e85' height="170px" width="270px">
  <img src='https://github.com/user-attachments/assets/9472e970-c083-4b31-9252-3eeecc761078' height="170px" width="270px">
</div>


ç›®å‰ï¼Œåœ¨ NVIDIA L20, RTX 4090 å’Œ RTX 3080 Laptop ä¸Šï¼Œä¸ cuBLAS é»˜è®¤çš„ Tensor Cores æ•°å­¦ç®—æ³• `CUBLAS_GEMM_DEFAULT_TENSOR_OP` ç›¸æ¯”ï¼Œæœ¬ä»“åº“å®ç°çš„ `HGEMM (WMMA/MMA/CuTe)` (`è“è‰²`ğŸ”µ) å¯ä»¥è¾¾åˆ°å…¶ (`æ©™è‰²`ğŸŸ ) æ€§èƒ½çš„ `98%~100%`ã€‚æ›´å¤šè¯¦æƒ…è¯·æŸ¥çœ‹ [toy-hgemm libraryâš¡ï¸âš¡ï¸](./kernels/hgemm)ã€‚

|ğŸ“šç‰¹æ€§ |ğŸ“šç‰¹æ€§ |ğŸ“šç‰¹æ€§ |ğŸ“šç‰¹æ€§|
|:---:|:---:|:---:|:---:|
|âœ”ï¸CUDA/**Tensor Cores**|âœ”ï¸K ç»´åº¦å¾ªç¯|âœ”ï¸åˆ†å— Block(BMxBK)|âœ”ï¸åˆ†å— Threads(T 8x8)|
|âœ”ï¸WMMA(m16n16k16)|âœ”ï¸MMA(m16n8k16)|âœ”ï¸Pack LDST(128 bits)|âœ”ï¸SMEM Padding|
|âœ”ï¸Copy Async|âœ”ï¸åˆ†å— MMAs|âœ”ï¸åˆ†å— Warps|âœ”ï¸**å¤šé˜¶æ®µ(2~4)**|
|âœ”ï¸å¯„å­˜å™¨åŒç¼“å†²|âœ”ï¸**Block Swizzle**|âœ”ï¸**Warp Swizzle**|âœ”ï¸**SMEM Swizzle**(CuTe/MMA)|
|âœ”ï¸Collective Store(Shfl)|âœ”ï¸Layout NN|âœ”ï¸Layout TN|âœ”ï¸SGEMM FP32/TF32|

## Â©ï¸å¼•ç”¨ğŸ‰ğŸ‰

```BibTeX
@misc{hgemm-tensorcores-mma@2024,
  title={hgemm-tensorcores-mma: Write HGEMM from scratch using Tensor Cores with WMMA, MMA PTX and CuTe API.},
  url={https://github.com/xlite-dev/hgemm-tensorcores-mma},
  note={Open-source software available at https://github.com/xlite-dev/hgemm-tensorcores-mma},
  author={xlite-dev etc},
  year={2024}
}
```

## ğŸ“– Toy-HGEMM åº“ä¸­çš„ HGEMM CUDA å†…æ ¸ ğŸ‰ğŸ‰

<div id="kernels"></div>

```C++
void hgemm_naive_f16(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_sliced_k_f16(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_t_8x8_sliced_k_f16x4(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_t_8x8_sliced_k_f16x4_pack(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_t_8x8_sliced_k_f16x4_bcf(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_t_8x8_sliced_k_f16x4_pack_bcf(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_t_8x8_sliced_k_f16x8_pack_bcf(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_t_8x8_sliced_k_f16x8_pack_bcf_dbuf(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_t_8x8_sliced_k16_f16x8_pack_dbuf(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_t_8x8_sliced_k16_f16x8_pack_dbuf_async(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_t_8x8_sliced_k32_f16x8_pack_dbuf(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_t_8x8_sliced_k32_f16x8_pack_dbuf_async(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_t_16x8_sliced_k32_f16x8_pack_dbuf(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_t_16x8_sliced_k32_f16x8_pack_dbuf_async(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_cublas_tensor_op_nn(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_cublas_tensor_op_tn(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_wmma_m16n16k16_naive(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_wmma_m16n16k16_mma4x2(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_wmma_m16n16k16_mma4x2_warp2x4(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_wmma_m16n16k16_mma4x2_warp2x4_dbuf_async(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_wmma_m32n8k16_mma2x4_warp2x4_dbuf_async(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_wmma_m16n16k16_mma4x2_warp2x4_stages(torch::Tensor a, torch::Tensor b, torch::Tensor c, int stages, bool swizzle, int swizzle_stride);
void hgemm_wmma_m16n16k16_mma4x2_warp2x4_stages_dsmem(torch::Tensor a, torch::Tensor b, torch::Tensor c, int stages, bool swizzle, int swizzle_stride);
void hgemm_wmma_m16n16k16_mma4x2_warp4x4_stages_dsmem(torch::Tensor a, torch::Tensor b, torch::Tensor c, int stages, bool swizzle, int swizzle_stride);
void hgemm_wmma_m16n16k16_mma4x4_warp4x4_stages_dsmem(torch::Tensor a, torch::Tensor b, torch::Tensor c, int stages, bool swizzle, int swizzle_stride);
void hgemm_mma_m16n8k16_naive(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_mma_m16n8k16_mma2x4_warp4x4(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_mma_m16n8k16_mma2x4_warp4x4_stages(torch::Tensor a, torch::Tensor b, torch::Tensor c, int stages, bool swizzle, int swizzle_stride);
void hgemm_mma_m16n8k16_mma2x4_warp4x4_stages_dsmem(torch::Tensor a, torch::Tensor b, torch::Tensor c, int stages, bool swizzle, int swizzle_stride);
void hgemm_mma_m16n8k16_mma2x4_warp4x4x2_stages_dsmem(torch::Tensor a, torch::Tensor b, torch::Tensor c, int stages, bool swizzle, int swizzle_stride);
void hgemm_mma_m16n8k16_mma2x4_warp4x4x2_stages_dsmem_x4(torch::Tensor a, torch::Tensor b, torch::Tensor c, int stages, bool swizzle, int swizzle_stride);
void hgemm_mma_m16n8k16_mma2x4_warp4x4x2_stages_dsmem_rr(torch::Tensor a, torch::Tensor b, torch::Tensor c, int stages, bool swizzle, int swizzle_stride);
void hgemm_mma_m16n8k16_mma2x4_warp4x4_stages_dsmem_tn(torch::Tensor a, torch::Tensor b, torch::Tensor c, int stages, bool swizzle, int swizzle_stride);
void hgemm_mma_stages_block_swizzle_tn_cute(torch::Tensor a, torch::Tensor b, torch::Tensor c, int stages, bool swizzle, int swizzle_stride);
void hgemm_mma_m16n8k16_mma2x4_warp4x4x2_stages_dsmem_swizzle(torch::Tensor a, torch::Tensor b, torch::Tensor c, int stages, bool swizzle, int swizzle_stride);
void hgemm_mma_m16n8k16_mma2x4_warp4x4x2_stages_dsmem_tn_swizzle_x4(torch::Tensor a, torch::Tensor b, torch::Tensor c, int stages, bool swizzle, int swizzle_stride);
```

## ğŸ“– ç›®å½•

- [ğŸ“– å‰ææ¡ä»¶](#prerequisites)
- [ğŸ“– å®‰è£…](#install)
- [ğŸ“– Python æµ‹è¯•](#test)
- [ğŸ“– C++ æµ‹è¯•](#test-cpp)
- [ğŸ“– NVIDIA L20 æ€§èƒ½æµ‹è¯•](#perf-l20)
- [ğŸ“– NVIDIA RTX 4090 æ€§èƒ½æµ‹è¯•](#perf-4090)
- [ğŸ“– NVIDIA RTX 3080 Laptop æ€§èƒ½æµ‹è¯•](#perf-3080)
- [ğŸ“– æ€§èƒ½ä¼˜åŒ–ç¬”è®°](#opt-docs)
- [ğŸ“– å‚è€ƒèµ„æ–™](#ref)

## ğŸ“– å‰ææ¡ä»¶
<div id="prerequisites"></div>

- PyTorch >= 2.0, CUDA >= 12.0
- æ¨è: PyTorch 2.5.1, CUDA 12.5

## ğŸ“– å®‰è£…

<div id="install"></div>

æœ¬ä»“åº“å®ç°çš„ HGEMM å¯ä»¥ä½œä¸ºä¸€ä¸ª Python åº“å®‰è£…ï¼Œå³ `toy-hgemm` åº“ (å¯é€‰)ã€‚
```bash
cd kernels/hgemm
git submodule update --init --recursive --force # Fetch `CUTLASS` submoduleï¼Œ needed
python3 setup.py bdist_wheel && cd dist && python3 -m pip install *.whl # pip uninstall toy-hgemm -y
```

## ğŸ“– Python æµ‹è¯•

<div id="test"></div>

**CUTLASS**: Fetch `CUTLASS` submodule. ç›®å‰ï¼Œæˆ‘ä½¿ç”¨ `v3.5.1` ç”¨äº HGEMM CuTe å†…æ ¸ã€‚
```bash
git submodule update --init --recursive --force
```

æ‚¨å¯ä»¥é€šè¿‡ Python è„šæœ¬æµ‹è¯•è®¸å¤šè‡ªå®šä¹‰ HGEMM å†…æ ¸ï¼Œå¹¶æ‰¾å‡ºå®ƒä»¬æ€§èƒ½ä¸Šçš„å·®å¼‚ã€‚

```bash
# æ‚¨å¯ä»¥åªæµ‹è¯• Ada æˆ– Ampereï¼Œä¹Ÿå¯ä»¥æµ‹è¯• Volta, Ampere, Ada, Hopper, ...
export TORCH_CUDA_ARCH_LIST=Ada # ä»…ç”¨äº Ada
export TORCH_CUDA_ARCH_LIST=Ampere # ä»…ç”¨äº Ampere
python3 hgemm.py --wmma # æµ‹è¯•æ‰€æœ‰ MNK çš„é»˜è®¤ wmma å†…æ ¸
python3 hgemm.py --mma  # æµ‹è¯•æ‰€æœ‰ MNK çš„é»˜è®¤ mma å†…æ ¸
python3 hgemm.py --M 16384 --N 16384 --K 8192 --i 10 --wmma # æµ‹è¯•ç‰¹å®š MNK çš„é»˜è®¤ wmma å†…æ ¸
python3 hgemm.py --M 16384 --N 16384 --K 8192 --i 10 --mma # æµ‹è¯•ç‰¹å®š MNK çš„é»˜è®¤ mma å†…æ ¸
python3 hgemm.py --wmma-all # æµ‹è¯•æ‰€æœ‰ MNK çš„æ‰€æœ‰ wmma å†…æ ¸
python3 hgemm.py --mma-all # æµ‹è¯•æ‰€æœ‰ MNK çš„æ‰€æœ‰ mma å†…æ ¸
python3 hgemm.py --cuda-all --wmma-all --mma-all # æµ‹è¯•æ‰€æœ‰ MNK çš„æ‰€æœ‰å†…æ ¸
python3 hgemm.py --cute-tn --no-default # æµ‹è¯•æ‰€æœ‰ MNK çš„å¸¦æœ‰ smem swizzle çš„ cute hgemm å†…æ ¸
```
å¦‚æœæ‚¨æƒ³ç»˜åˆ¶ TFLOPS æ›²çº¿ï¼Œæ‚¨éœ€è¦å…ˆå®‰è£… `matplotlib` å¹¶è®¾ç½® --plot-flops (æˆ– --plot) é€‰é¡¹ã€‚
```bash
python3 -m pip install matplotlib
# æŒ‡å®š topk åªç»˜åˆ¶æ€§èƒ½æœ€å¥½çš„å‰ k ä¸ªå†…æ ¸ã€‚
python3 hgemm.py --mma-all --plot --topk 8
# æµ‹è¯•é»˜è®¤ mma å†…æ ¸å’Œå¸¦æœ‰ smem swizzle çš„ cute hgemm å†…æ ¸ï¼Œé€‚ç”¨äºæ‰€æœ‰ MNK
python3 hgemm.py --cute-tn --mma --plot
```

## ğŸ“– C++ æµ‹è¯•

<div id="test-cpp"></div>

HGEMM åŸºå‡†æµ‹è¯•ä¹Ÿæ”¯æŒ C++ æµ‹è¯•ã€‚ç›®å‰ï¼Œå®ƒæ”¯æŒä»¥ä¸‹å®ç°ä¹‹é—´çš„æ¯”è¾ƒï¼š

- æœ¬ä»“åº“å®ç°çš„ MMA HGEMM NN
- æœ¬ä»“åº“å®ç°çš„ CuTe HGEMM TN
- ä½¿ç”¨é»˜è®¤ Tensor Cores æ•°å­¦ç®—æ³•çš„ cuBLAS HGEMM TN

ä» C++ äºŒè¿›åˆ¶æµ‹è¯•è·å¾—æ€§èƒ½æ•°æ®å¾€å¾€æ¯” Python æµ‹è¯•ç•¥å¥½ã€‚è¿™ç§å·®å¼‚å¯èƒ½å½’å› äº PyTorch Python ç»‘å®šå¼•å…¥çš„é¢å¤–å¼€é”€ã€‚
```bash
make
./hgemm_mma_stage.bin
# NVIDIA L20
ALGO = MMA16816 HGEMM NN MMA=2x4 WARP=4x4x2 STAGES=2 BLOCK SWIZZLE=2048
M N K =  12544  12544  12544, Time =   0.03445555   0.03446098   0.03447399 s, AVG Performance =   114.5541 Tflops
M N K =  15360  15360  15360, Time =   0.06307226   0.06307789   0.06308864 s, AVG Performance =   114.9017 Tflops
M N K =  15616  15616  15616, Time =   0.06612480   0.06612798   0.06613094 s, AVG Performance =   115.1739 Tflops
M N K =  15872  15872  15872, Time =   0.06969549   0.06970215   0.06971290 s, AVG Performance =   114.7305 Tflops
M N K =  16128  16128  16128, Time =   0.07295078   0.07295406   0.07295693 s, AVG Performance =   115.0064 Tflops
M N K =  16384  16384  16384, Time =   0.07663001   0.07663534   0.07664947 s, AVG Performance =   114.7785 Tflops

./hgemm_cute.bin
# NVIDIA L20
ALGO = CuTe HGEMM, TN, STAGES=2, SMEM SWIZZLE=<3, 3, 3>, BLOCK SWIZZLE=2048
M N K =  12544  12544  12544, Time =   0.03413504   0.03414354   0.03415450 s, AVG Performance =   115.6191 Tflops
M N K =  15360  15360  15360, Time =   0.06227354   0.06228111   0.06228992 s, AVG Performance =   116.3717 Tflops
M N K =  15616  15616  15616, Time =   0.06492467   0.06493727   0.06496666 s, AVG Performance =   117.2858 Tflops
M N K =  15872  15872  15872, Time =   0.06843085   0.06843873   0.06844723 s, AVG Performance =   116.8485 Tflops
M N K =  16128  16128  16128, Time =   0.07200256   0.07200881   0.07201792 s, AVG Performance =   116.5161 Tflops
M N K =  16384  16384  16384, Time =   0.07564493   0.07565752   0.07567462 s, AVG Performance =   116.2620 Tflops

./hgemm_cublas.bin
# NVIDIA L20
ALGO = cuBLAS CUBLAS_GEMM_DEFAULT_TENSOR_OP TN
M N K =  12544  12544  12544, Time =   0.03472691   0.03472968   0.03473408 s, AVG Performance =   113.6678 Tflops
M N K =  15360  15360  15360, Time =   0.06332416   0.06333143   0.06334157 s, AVG Performance =   114.4417 Tflops
M N K =  15616  15616  15616, Time =   0.06649446   0.06650184   0.06651699 s, AVG Performance =   114.5264 Tflops
M N K =  15872  15872  15872, Time =   0.06977024   0.06977659   0.06978355 s, AVG Performance =   114.6081 Tflops
M N K =  16128  16128  16128, Time =   0.07319142   0.07320709   0.07326925 s, AVG Performance =   114.6089 Tflops
M N K =  16384  16384  16384, Time =   0.07668429   0.07669371   0.07670784 s, AVG Performance =   114.6912 Tflops
```

## ğŸ“– æ€§èƒ½æµ‹è¯•

<div id="perf-l20"></div>

### ğŸ“– NVIDIA L20
<!--
ç›®å‰æœ€ä¼˜çš„å®ç°ï¼Œåœ¨L20ä¸Šï¼ˆç†è®ºTensor Cores FP16ç®—åŠ›ä¸º 119.5 TFLOPSï¼‰ï¼Œæ•´ä½“ä¸Šèƒ½è¾¾åˆ°cuBLASå¤§æ¦‚`99~100+%`å·¦å³çš„æ€§èƒ½ã€‚ä½¿ç”¨WMMA APIèƒ½è¾¾åˆ°cuBLASå¤§æ¦‚`95%~98%`å·¦å³çš„æ€§èƒ½(105-113 TFLOPS vs 105-115 TFLOPS)ï¼Œä½¿ç”¨MMA APIèƒ½è¾¾åˆ°115 TFLOPSï¼Œéƒ¨åˆ† case ä¼šè¶…è¶Š cuBLASã€‚CuTe ç‰ˆæœ¬çš„ HGEMM å®ç°äº† Block Swizzleï¼ˆL2 Cache friendlyï¼‰å’Œ SMEM Swizzleï¼ˆbank conflicts freeï¼‰ï¼Œæ€§èƒ½æœ€ä¼˜ï¼Œå¤§è§„æ¨¡çŸ©é˜µä¹˜èƒ½è¾¾åˆ° 116-117 TFLOPSï¼Œæ˜¯ cuBLAS å¤§æ¦‚`98%~100%+`å·¦å³çš„æ€§èƒ½ï¼Œå¾ˆå¤šcaseä¼šè¶…è¶ŠcuBLASã€‚ç›®å‰é€šè¿‡ SMEM Padding å’Œ SMEM Swizzle çš„æ–¹å¼ç¼“è§£ bank conflictsã€‚å¯¹äº NN layoutï¼Œä½¿ç”¨ SMEM Padding ç¼“è§£ bank conflictsï¼›å¯¹äº TN layoutï¼Œé€šè¿‡ CUTLASS/CuTe çš„ SMEM Swizzle æ¶ˆé™¤ bank conflictsã€‚
-->
ç›®å‰æœ€ä¼˜çš„å®ç°ï¼Œåœ¨ L20 ä¸Šï¼ˆç†è®º Tensor Cores FP16 ç®—åŠ›ä¸º 119.5 TFLOPSï¼‰ï¼Œæ•´ä½“ä¸Šèƒ½è¾¾åˆ° cuBLAS å¤§æ¦‚ `99%~100+%` å·¦å³çš„æ€§èƒ½ã€‚

- ä½¿ç”¨ WMMA APIï¼Œèƒ½è¾¾åˆ° cuBLAS å¤§æ¦‚ `95%~98%` å·¦å³çš„æ€§èƒ½ (105-113 TFLOPS vs 105-115 TFLOPS)ã€‚
- ä½¿ç”¨ MMA APIï¼Œèƒ½è¾¾åˆ° 115 TFLOPSï¼Œéƒ¨åˆ† case ä¼šè¶…è¶Š cuBLASã€‚
- CuTe ç‰ˆæœ¬çš„ HGEMM å®ç°äº† Block Swizzleï¼ˆL2 Cache friendlyï¼‰å’Œ SMEM Swizzleï¼ˆbank conflicts freeï¼‰ï¼Œæ€§èƒ½æœ€ä¼˜ã€‚å¤§è§„æ¨¡çŸ©é˜µä¹˜èƒ½è¾¾åˆ° 116-117 TFLOPSï¼Œæ˜¯ cuBLAS å¤§æ¦‚ `98%~100%+` å·¦å³çš„æ€§èƒ½ï¼Œå¾ˆå¤š case ä¼šè¶…è¶Š cuBLASã€‚

ç›®å‰é€šè¿‡ SMEM Padding å’Œ SMEM Swizzle çš„æ–¹å¼ç¼“è§£ bank conflictsï¼š

- å¯¹äº NN layoutï¼Œä½¿ç”¨ SMEM Padding ç¼“è§£ bank conflictsã€‚
- å¯¹äº TN layoutï¼Œé€šè¿‡ CUTLASS/CuTe çš„ SMEM Swizzle æ¶ˆé™¤ bank conflictsã€‚

<div id="NV-L20"></div>


![NVIDIA_L20_NN+TN+v2](https://github.com/user-attachments/assets/71927ac9-72b3-4ce9-b0e2-788b5885bc99)


æµ‹è¯•æ‰€æœ‰ MNK è®¾ç½®çš„å‘½ä»¤ (æç¤º: å•ç‹¬æµ‹è¯•æ¯ä¸ª MNK çš„æ€§èƒ½æ•°æ®æ›´å‡†ç¡®ã€‚)
```bash
python3 hgemm.py --cute-tn --mma --plot
```

### ğŸ“– NVIDIA GeForce RTX 4090

<div id="perf-4090"></div>

<!--
åœ¨NVIDIA RTX 4090ä¸Š(FP16 Tensor Coresç®—åŠ›ä¸º330 TFLOPS)ï¼ŒWMMA(m16n16k16)æ€§èƒ½è¡¨ç°æ¯”MMA(m16n8k16)è¦æ›´å¥½ï¼Œå¤§åˆ†éƒ¨MNKä¸‹ï¼Œæœ¬ä»“åº“çš„å®ç°èƒ½è¾¾åˆ°cuBLAS 95%~99%çš„æ€§èƒ½ï¼ŒæŸäº›caseèƒ½è¶…è¿‡cuBLASã€‚å°±æœ¬ä»“åº“çš„å®ç°è€Œè¨€ï¼Œåœ¨RTX 4090ä¸Šï¼Œå¤§è§„æ¨¡çŸ©é˜µä¹˜(MNK>=8192)ï¼ŒWMMAè¡¨ç°æ›´ä¼˜ï¼Œå°è§„æ¨¡çŸ©é˜µä¹˜ï¼ŒMMAè¡¨ç°æ›´ä¼˜ã€‚
-->

åœ¨ NVIDIA RTX 4090 ä¸Š (FP16 Tensor Cores ç®—åŠ›ä¸º 330 TFLOPS)ï¼ŒWMMA (m16n16k16) å®ç°çš„æ€§èƒ½è¡¨ç°æ¯” MMA (m16n8k16) æ›´å¥½ã€‚å¯¹äºå¤§å¤šæ•° MNK é…ç½®ï¼Œæœ¬ä»“åº“çš„å®ç°èƒ½è¾¾åˆ° cuBLAS 95%~99% çš„æ€§èƒ½ï¼ŒæŸäº› case èƒ½è¶…è¿‡ cuBLASã€‚å…·ä½“æ¥è¯´ï¼š

- å¯¹äºå¤§è§„æ¨¡çŸ©é˜µä¹˜æ³• (MNK >= 8192)ï¼ŒWMMA å®ç°è¡¨ç°æ›´ä¼˜ã€‚
- å¯¹äºå°è§„æ¨¡çŸ©é˜µä¹˜æ³•ï¼ŒMMA å®ç°æ›´é«˜æ•ˆã€‚


![NVIDIA_GeForce_RTX_4090_NN+TN+v4](https://github.com/user-attachments/assets/05ef4f5e-d999-48ea-b58e-782cffb24e85)

```bash
python3 hgemm.py --cute-tn --mma --wmma-all --plot
```

### ğŸ“– NVIDIA GeForce RTX 3080 Laptop

<div id="perf-3080"></div>

<!--
åœ¨NVIDIA GeForce RTX 3080 Laptopä¸Šæµ‹è¯•ï¼Œä½¿ç”¨mma4x4_warp4x4ï¼ˆ16 WMMA m16n16k16 ops, warp tile 64x64ï¼‰ä»¥åŠThread block swizzleï¼Œå¤§éƒ¨åˆ†caseèƒ½æŒå¹³ç”šè‡³è¶…è¿‡cuBLASï¼Œä½¿ç”¨Windows WSL2 + RTX 3080 Laptopè¿›è¡Œæµ‹è¯•ã€‚
-->
åœ¨ NVIDIA GeForce RTX 3080 Laptop ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œä½¿ç”¨ mma4x4_warp4x4 é…ç½®ï¼ˆåŒ…æ‹¬ 16 ä¸ª WMMA m16n16k16 æ“ä½œï¼Œwarp tile å¤§å°ä¸º 64x64ï¼‰ä»¥åŠ Thread block swizzleã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæ­¤è®¾ç½®å¯ä»¥è¾¾åˆ°ç”šè‡³è¶…è¿‡ cuBLAS çš„æ€§èƒ½ã€‚æµ‹è¯•æ˜¯ä½¿ç”¨ Windows WSL2 + RTX 3080 Laptop è¿›è¡Œçš„ã€‚

![image](https://github.com/user-attachments/assets/9472e970-c083-4b31-9252-3eeecc761078)

```bash
python3 hgemm.py --wmma-all --plot
```

<details>
<summary> ğŸ”‘ï¸ æ€§èƒ½ä¼˜åŒ–ç¬”è®°(TODO)</summary>

## ğŸ“– æ€§èƒ½ä¼˜åŒ–ç¬”è®°

<div id="opt-docs"></div>

### PyTorch HGEMM Profile

åœ¨ Ada æ¶æ„ä¸‹ï¼ŒPyTorch 2.4 ä½¿ç”¨ matmul æ—¶ï¼Œä¼šè°ƒç”¨:
```C++
ampere_fp16_s1688gemm_fp16_128x128_ldg8_f2f_stages_32x1_nn_kernel
```
å†…éƒ¨å®é™…ä½¿ç”¨ HMMA(Tensor Cores) è¿›è¡Œè®¡ç®—ï¼Œåœ¨ 3080 ä¸Š profile å‘ç°ä½¿ç”¨:
```C++
sm80_xmma_gemm_f16f16_f16f32_f32_nn_n_tilesize96x64x32_stage3_warpsize2x2x1_tensor16x8x16_kernel
```
å› æ­¤ï¼Œåªæœ‰å®ç°ä½¿ç”¨ Tensor Cores çš„ HGEMMï¼Œæ‰æœ‰å¯èƒ½æ¥è¿‘ PyTorch/cuBLAS çš„æ€§èƒ½ã€‚
```bash
ncu -o hgemm.prof -f python3 bench/prof.py
nsys profile --stats=true -t cuda,osrt,nvtx -o hgemm.prof --force-overwrite true python3 prof.py
```
- SASS (L20)

```C
// ampere_fp16_s1688gemm_fp16_128x128_ldg8_f2f_stages_32x1_nn_kernel
310	00007f41 37d5b850	      LDSM.16.M88.4 R192, [R169+UR8+0x2000]
311	00007f41 37d5b860	      LDSM.16.M88.4 R196, [R169+UR8+0x2800]
336	00007f41 37d5b9f0	      HMMA.1688.F32 R112, R182, R196, R112
...
```

### SMEM Padding

#### Bank Conflicts çš„äº§ç”Ÿ

å«ä¹‰ï¼šåœ¨è®¿é—® shared memory æ—¶ï¼Œå› å¤šä¸ªçº¿ç¨‹è¯»å†™åŒä¸€ä¸ª Bank ä¸­çš„ä¸åŒæ•°æ®åœ°å€æ—¶ï¼Œå¯¼è‡´ shared memory å¹¶å‘è¯»å†™ é€€åŒ– æˆé¡ºåºè¯»å†™çš„ç°è±¡å«åš Bank Conflictï¼›

![](https://github.com/PaddleJitLab/CUDATutorial/blob/develop/docs/09_optimize_reduce/02_bank_conflict/images/ef322be7c3e5b6b9be69d2b90e88083f50569a58a97129f348e483b946ab4edf.png)

SM è°ƒåº¦å•ä½ä¸ºä¸€ä¸ª warpï¼ˆä¸€ä¸ª warp å†… 32 ä¸ª Threadï¼‰ï¼Œshared_memory å¯ä»¥ è¢«ä¸€ä¸ª warp ä¸­çš„æ‰€æœ‰ï¼ˆ32 ä¸ªï¼‰çº¿ç¨‹è¿›è¡Œè®¿é—®ï¼Œshared_memory æ˜ å°„åˆ°å¤§å°ç›¸ç­‰çš„ 32 ä¸ª Bank ä¸Šï¼ŒBank çš„æ•°æ®è¯»å–å¸¦å®½ä¸º 32bit / cycle (4 bytes)ï¼Œå› æ­¤ï¼Œä¸»è¦éœ€è¦è€ƒè™‘ä¸€ä¸ª Warp å†… 32 çº¿ç¨‹çš„è®¿é—®å…±äº«å†…å­˜æ—¶çš„ bank å†²çªã€‚
å¯¹äºå¤šä¸ªçº¿ç¨‹è¯»å–åŒä¸€ä¸ª Bank æ•°æ®æ—¶ï¼ˆä¸åŒåœ°å€ï¼‰ï¼Œç¡¬ä»¶æŠŠå†…å­˜è¯»å†™è¯·æ±‚ï¼Œæ‹†åˆ†æˆ conflict-free requestsï¼Œè¿›è¡Œé¡ºåºè¯»å†™ï¼Œæ­¤æ—¶å°†ä¼šè§¦å‘å¤šæ¬¡å†…å­˜äº‹åŠ¡ã€‚ç‰¹åˆ«åœ°ï¼Œå½“ä¸€ä¸ª warp ä¸­çš„æ‰€æœ‰çº¿ç¨‹è¯»å†™åŒä¸€ä¸ªåœ°å€æ—¶ï¼Œä¼šè§¦å‘ broadcast æœºåˆ¶ï¼Œæ­¤æ—¶ä¸ä¼šé€€åŒ–æˆé¡ºåºè¯»å†™ã€‚ä¸Šé¢æåˆ°è§¦å‘ broadcast æœºåˆ¶çš„æ¡ä»¶æ˜¯ all threads acess same addressï¼Œä½†åœ¨ç¿»é˜… cuda-c-programming-guide ä»¥åŠæœ€æ–°ç‰ˆæœ¬çš„[NVProfGuide](https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html) æ—¶ï¼Œå‘ç°åªè¦æ˜¯å¤šä¸ª thread è¯»å†™å°±ä¼šè§¦å‘ broadcastï¼ˆä¸éœ€è¦ Allï¼‰ã€‚

- å¤šä¸ªçº¿ç¨‹è¯»åŒä¸€ä¸ªæ•°æ®æ—¶ï¼Œä»…æœ‰ä¸€ä¸ªçº¿ç¨‹è¯»ï¼Œç„¶å broadcast åˆ°å…¶ä»–çº¿ç¨‹
- å¤šä¸ªçº¿ç¨‹å†™åŒä¸€ä¸ªæ•°æ®æ—¶ï¼Œä»…ä¼šæœ‰ä¸€ä¸ªçº¿ç¨‹å†™æˆåŠŸ

NVIDIA çš„[æ–‡ç« ](https://developer.nvidia.com/blog/using-shared-memory-cuda-cc/)ä¸­æŒ‡å‡ºï¼Œæˆ‘ä»¬è¿˜å¯ä»¥é€šè¿‡ `cudaDeviceSetSharedMemConfig()` å‡½æ•°è®¾ç½®é»˜è®¤ Bank Sizeï¼ˆé»˜è®¤ä¸º 4 bytesï¼‰æ¥é¿å… bank conflictsï¼Œå¯è®¾ç½®ä¸º cudaSharedMemBankSizeFourByte æˆ–è€… cudaSharedMemBankSizeEightByteã€‚å¯¹äºæŸäº›åœºæ™¯æ¥è¯´ï¼Œè®¾ç½® cudaSharedMemBankSizeEightByte æˆ–è®¸æ›´åŠ åˆé€‚ï¼Œæ¯”å¦‚ä½¿ç”¨ double æ•°æ®ç±»å‹æ—¶ã€‚

```C
cudaDeviceSetSharedMemConfig(cudaSharedMemBankSizeEightByte);
```
ç›®å‰é€šè¿‡ SMEM Padding å’Œ SMEM swizzle çš„æ–¹å¼ç¼“è§£ bank conflictsã€‚å¯¹äº NN layoutï¼Œä½¿ç”¨ SMEM Padding ç¼“è§£ bank conflictsï¼›å¯¹äº TN layoutï¼Œé€šè¿‡ cutlass cute çš„ SMEM Swizzle æ¶ˆé™¤ bank conflictsã€‚

### åŒç¼“å†² Double Buffers

æœ¬ä»“åº“å®ç°çš„ HGEMM Double Buffers ç­–ç•¥å¦‚ä¸‹ï¼š1ï¼‰ä¸»å¾ªç¯ä» bk = 1 å¼€å§‹ï¼Œç¬¬ä¸€æ¬¡æ•°æ®åŠ è½½åœ¨ä¸»å¾ªç¯ä¹‹å‰ï¼Œæœ€åä¸€æ¬¡è®¡ç®—åœ¨ä¸»å¾ªç¯ä¹‹åï¼Œè¿™æ˜¯ pipeline çš„ç‰¹ç‚¹å†³å®šçš„ï¼›2ï¼‰ç”±äºè®¡ç®—å’Œä¸‹ä¸€æ¬¡è®¿å­˜ä½¿ç”¨çš„ Shared Memory ä¸åŒï¼Œå› æ­¤ä¸»å¾ªç¯ä¸­æ¯æ¬¡å¾ªç¯åªéœ€è¦ä¸€æ¬¡ __syncthreads() å³å¯ï¼Œå¯¹æ¯”é double buffers ç‰ˆæœ¬ï¼Œæ€»å…±èŠ‚çœäº† ((K + BK - 1) / BK) - 1 æ¬¡ block å†…çš„åŒæ­¥æ“ä½œã€‚æ¯”å¦‚ï¼Œbk=1 æ—¶ï¼ŒHFMA è®¡ç®—ä½¿ç”¨çš„æ˜¯ s_a[0] å’Œ s_b[0]ï¼Œå› æ­¤ï¼Œå’Œ s_a[1] å’Œ s_b[1] çš„åŠ è½½æ˜¯æ²¡æœ‰ä¾èµ–å…³ç³»çš„ã€‚HFMA è®¡ç®—ï¼Œä» global å†…å­˜åˆ° s_a[1] å’Œ s_b[1] å’Œ HFMA è®¡ç®—å¯ä»¥å¹¶è¡Œã€‚s_a[1] å’Œ s_b[1] ç”¨äºåŠ è½½ä¸‹ä¸€å— BK éœ€è¦çš„æ•°æ®åˆ°å…±äº«å†…å­˜ï¼›3ï¼‰ç”±äº GPU ä¸èƒ½å‘ CPU é‚£æ ·æ”¯æŒä¹±åºæ‰§è¡Œï¼Œä¸»å¾ªç¯ä¸­éœ€è¦å…ˆå°†ä¸‹ä¸€æ¬¡å¾ªç¯è®¡ç®—éœ€è¦çš„ Gloabal Memory ä¸­çš„æ•°æ® load åˆ°å¯„å­˜å™¨ï¼Œç„¶åè¿›è¡Œæœ¬æ¬¡è®¡ç®—ï¼Œä¹‹åå†å°† load åˆ°å¯„å­˜å™¨ä¸­çš„æ•°æ®å†™åˆ° Shared Memoryï¼Œè¿™æ ·åœ¨ LDG æŒ‡ä»¤å‘ Global Memory åš load æ—¶ï¼Œä¸ä¼šå½±å“åç»­ HFMA åŠå…¶å®ƒè¿ç®—æŒ‡ä»¤çš„ launch æ‰§è¡Œï¼Œä¹Ÿå°±è¾¾åˆ°äº† Double Buffers çš„ç›®çš„ï¼Œå…·ä½“ä»£ç è§[hgemm.cu](./hgemm.cu)ã€‚


### Tile Block

TODO

### Tile Thread

TODO

### Pack LDST 128 bits

TODO

### Async Copy

TODO

### Multi Stages

TODO

### Tensor Cores(WMMA/MMA)

TODO

### Tile MMA/Warp

TODO

### Thread Block Swizze

TODO

### Warp Swizzle

TODO

### Reg Double Buffers

TODO

### Collective Store(Reg Reuse&Warp Shuffle)

TODO

### SMEM Swizzle/Permuted

TODO

</details>

## ğŸ“– å‚è€ƒèµ„æ–™

<div id="ref"></div>

- [flash-attention-minimal](https://github.com/tspeterkim/flash-attention-minimal)
- [tiny-flash-attention](https://github.com/66RING/tiny-flash-attention)
- [cute-gemm](https://github.com/reed-lau/cute-gemm)
- [cutlass_flash_atten_fp8](https://github.com/weishengying/cutlass_flash_atten_fp8)
- [cuda_learning](https://github.com/ifromeast/cuda_learning)
- [cuda_hgemm](https://github.com/Bruce-Lee-LY/cuda_hgemm)
- [cuda-tensorcore-hgemm](https://github.com/nicolaswilde/cuda-tensorcore-hgemm)
- [How_to_optimize_in_GPU](https://github.com/Liu-xiandong/How_to_optimize_in_GPU/tree/master/sgemv)
- [cute_gemm](https://github.com/weishengying/cute_gemm)
- [cutlass](https://github.com/NVIDIA/cutlass)